This folder will contain R (or python) scripts for performing unsupervised learning on a 
screen sized data set.

The screen data is stored in $SCRATCH/screenData, and is drawn from the following 
location on bc:
/home/morphology/Morphology/Images/Screens_Rad52/1_Primary_Screens/1and2_SM_and_SMPhleo/RAD52GH_HTA2mCN_can1RPL39prtdTU_Rep1/Batches/Batch2_2011_05_11_Phleo0h_ProducingTiffsForNuclearMorphology_IPA_0p002_20Smooth20Suppress_15and40

A todo list of the project so far:

[Early work]
 
1) Transfer data (done!)
2) Transfer labels for the data as generated by the SVM foci classifier (done!)
3) Find or make R packages to do PCA on a whole screen's volume of data (done, scikits-learn can do this in batches)
4) Find of make R packages to do k-means out of core on scinet (done, scikits-learn will do this in batches)

[Middling work]

5) Figure best way to dump screen data in smaller files into one hdf5 file for better optimization on scinet's filesystem. (done with PyTables)

7) Evaluate performance of PCA on the screen data set.
	- Look at the decay in the eigenvalue spectrum
	- Does K-means clustering produce meaningful results on the data set when k is approximately equal to the number of significant eigenvalues?

8) Evaluate performance of K-means clustering on the data
	- Does K-means clustering produce meaningful results on the data set for any values of k?
	- Does a non-linear feature transformation of the data, followed by out-of-core K-means with euclidean distance yield better results for the same data set?

[Later work]

Depending on the results of 1-8, this will make the case for pursuing a more advanced model.  There are several possibilities:

A) One semi-supervised model.  Try the kernel learning method of Weinberger & Saul (solves an SDP based on optimizing a multi-class hinge loss function).  Originaly formulated to be used with k-nearest neighbour classification, this might be useful also for a spectral clustering (or weighted kernel K-means clustering).

B) A two stage co-training model.  The first stage would be unsupervised clustering (in some infinite mixture model / nonparametric bayesian sense), averaged over several runs.  It would be nice if the clustering was exemplar based, but this is not a deal breaker.  This would be followed by sampling from the clusters to examine the phenotypes present, followed by multi-way nearest neighbour based classification, using the clustering solution (or subsamples from the clustering solution) as labels for training.


 
